<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Transformer LLM</title>
    <link href="/2024/12/23/TransformerLLM/"/>
    <url>/2024/12/23/TransformerLLM/</url>
    
    <content type="html"><![CDATA[<h1 id="TransformerLLM"><a href="#TransformerLLM" class="headerlink" title="TransformerLLM"></a>TransformerLLM</h1><p>Analysis of Weekly Papers on <strong>Transformer and LLM</strong> for <strong>Image and Video Generation</strong>.</p><ul><li><a href="#introduction">Introduction</a><ul><li><a href="#long-context-processing">Long-Context Processing</a><ul><li><a href="#transformerfam-feedback-attention-is-working-memory">TransformerFAM: Feedback attention is working memory</a></li><li><a href="#ruler-whats-the-real-context-size-of-your-long-context-language-models">RULER: What‚Äôs the Real Context Size of Your Long-Context Language Models?</a></li><li><a href="#lloco-learning-long-contexts-offline">LLoCO: Learning Long Contexts Offline</a></li><li><a href="#better--faster-large-language-models-via-multi-token-prediction">Better &amp; Faster Large Language Models via Multi-token Prediction</a></li><li><a href="#koala-key-frame-conditioned-long-video-llm">Koala: Key frame-conditioned long video-LLM</a></li><li><a href="#ma-lmm-memory-augmented-large-multimodal-model-for-long-term-video-understanding">MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</a></li><li><a href="#moviechat-from-dense-token-to-sparse-memory-for-long-video-understanding">MovieChat: From Dense Token to Sparse Memory for Long Video Understanding</a></li><li><a href="#megalodon-efficient-llm-pretraining-and-inference-with-unlimited-context-length">Megalodon Efficient LLM Pretraining and Inference with Unlimited Context Length</a></li><li><a href="#triforce-lossless-acceleration-of-long-sequence-generation-with-hierarchical-speculative-decoding">TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding</a></li></ul></li><li><a href="#scaling">Scaling</a><ul><li><a href="#ivideogpt-interactive-videogpts-are-scalable-world-models">iVideoGPT: Interactive VideoGPTs are Scalable World Models</a></li><li><a href="#beyond-scaling-laws-understanding-transformer-performance-with-associative-memory">Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory</a></li><li><a href="#diffscaler-enhancing-the-generative-prowess-of-diffusion-transformers">Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers</a></li><li><a href="#stacking-your-transformers-a-closer-look-at-model-growth-for-efficient-llm-pre-training">Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training</a></li></ul></li><li><a href="#camera-control">camera control</a><ul><li><a href="#vd3d-taming-large-video-diffusion-transformers-for-3d-camera-control">VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</a></li></ul></li><li><a href="#performance--others">Performance &amp; Others</a><ul><li><a href="#what-matters-when-building-vision-language-models">What matters when building vision-language models?</a></li><li><a href="#not-all-language-model-features-are-linear">Not All Language Model Features Are Linear</a></li><li><a href="#llm2vec-large-language-models-are-secretly-powerful-text-encoders">LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</a></li><li><a href="#adapting-llama-decoder-to-vision-transformer">Adapting LLaMA Decoder to Vision Transformer</a></li><li><a href="#rho-1-not-all-tokens-are-what-you-need">RHO-1: Not All Tokens Are What You Need</a></li></ul></li></ul></li></ul><h2 id="Long-Context-Processing"><a href="#Long-Context-Processing" class="headerlink" title="Long-Context Processing"></a>Long-Context Processing</h2><h3 id="TransformerFAM-Feedback-attention-is-working-memory"><a href="#TransformerFAM-Feedback-attention-is-working-memory" class="headerlink" title="TransformerFAM: Feedback attention is working memory"></a>TransformerFAM: Feedback attention is working memory</h3><ul><li><code>Keypoints:</code> Feedback Attention Memory (FAM)„ÄÅ Working Memory in Transformers„ÄÅLong-Context Processing„ÄÅEfficiency and Integration</li><li><code>Objective:</code>The paper aims to address the limitation of Transformers in processing long sequences due to their quadratic attention complexity. The goal is to enable Transformers to handle indefinitely long inputs by introducing a novel architecture that mimics working memory mechanisms found in the human brain.</li><li><details><summary>Details</summary><ul><li><code>Method:</code><br>The authors propose the TransformerFAM architecture, which integrates a feedback loop to allow the network to attend to its own latent representations. This design introduces a working memory component that compresses and propagates information over an indefinite horizon without additional weights, thus maintaining past information for long contexts. TransformerFAM is designed to be compatible with pre-trained models and is tested across various model sizes.</li><li><code>Results:</code><br>The effectiveness of TransformerFAM is evaluated through significant improvements on long-context tasks across different model sizes (1B, 8B, and 24B). The results demonstrate that TransformerFAM outperforms standard Transformer models and Block Sliding Window Attention (BSWA) models on tasks requiring long-term contextual understanding, showcasing its potential for empowering Large Language Models (LLMs) to process sequences of unlimited length.</details></li></ul></li></ul><h3 id="RULER-What‚Äôs-the-Real-Context-Size-of-Your-Long-Context-Language-Models"><a href="#RULER-What‚Äôs-the-Real-Context-Size-of-Your-Long-Context-Language-Models" class="headerlink" title="RULER: What‚Äôs the Real Context Size of Your Long-Context Language Models?"></a>RULER: What‚Äôs the Real Context Size of Your Long-Context Language Models?</h3><ul><li><p><code>Keypoints:</code> Long-context LMs; Synthetic Benchmark;</p></li><li><p><code>Objective:</code>To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity.</p></li><li><details><summary>Details</summary><ul><li><code>Method:</code><ul><li>We propose a new benchmark RULER for evaluating long-context language models via synthetic tasks with flexible configurations.</li><li>We introduce new task categories, specifically multi-hop tracing and aggregation, to test behaviors other than retrieval from long context. </li><li>We evaluate ten long-context LMs using RULER and perform analysis across models and task complexities.  </details></li></ul></li></ul></li></ul><h3 id="LLoCO-Learning-Long-Contexts-Offline"><a href="#LLoCO-Learning-Long-Contexts-Offline" class="headerlink" title="LLoCO: Learning Long Contexts Offline"></a>LLoCO: Learning Long Contexts Offline</h3><ul><li><code>Keypoints:</code> Long-context LMs; Synthetic Benchmark;</li><li><code>Objective:</code> Their method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately.</li><li><details><summary>Details</summary><ul><li><code>Method:</code> They introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. Their approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens.</li><li><code>Metric:</code> In the experiment section, they aim to investigate the following aspects of LLoCO: (1) its effectiveness in comprehending compressed long contexts, (2) the extent to which summary embeddings can preserve essential information, and (3) the associated system costs.</li></ul></li></ul></details><h3 id="Better-Faster-Large-Language-Models-via-Multi-token-Prediction"><a href="#Better-Faster-Large-Language-Models-via-Multi-token-Prediction" class="headerlink" title="Better &amp; Faster Large Language Models via Multi-token Prediction"></a>Better &amp; Faster Large Language Models via Multi-token Prediction</h3><ul><li><code>Keypoints:</code>  LLM; Multi-token Prediction;</li><li><code>Objective:</code>In this work, they suggest that training language models to predict multiple future tokens at once results leading to higher sample efficiency.</li></ul><p align="center">  <img src="https://github.com/user-attachments/assets/8371df83-d9c0-4087-b415-658e6304dfff" width="200" />  <img src="https://github.com/user-attachments/assets/d6b00bb9-cf69-4af0-9543-2e9fe1c5a9f4" width="450" /></p><h3 id="Koala-Key-frame-conditioned-long-video-LLM"><a href="#Koala-Key-frame-conditioned-long-video-LLM" class="headerlink" title="Koala: Key frame-conditioned long video-LLM"></a>Koala: Key frame-conditioned long video-LLM</h3><ul><li><code>Keypoints:</code>Long Video Understanding; Key Frame-Conditioned LLM; Sparsely Sampled Key Frames; Zero-Shot Long Video QA;</li><li><code>Objective:</code>The research aims to enhance the capability of Large Language Models (LLMs), specifically video-based (vLLMs), to understand and answer questions about long videos (minutes-long) by addressing the limitations of existing models trained on short video clips.</li><li><details><summary>Details</summary><ul><li><p><code>Method:</code> The proposed approach, Koala, introduces a lightweight and self-supervised method that uses learnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to longer videos. It employs sparsely sampled key frames to condition the LLM, allowing it to focus on relevant regions in the input frames and make more informed predictions. The method introduces two new tokenizers that condition on visual tokens computed from these key frames for understanding both short and long video moments.</p></li><li><p><code>Metric:</code> The effectiveness of Koala is tested on zero-shot long video understanding benchmarks, including EgoSchema and SeedBench. The results show that Koala outperforms state-of-the-art large models by 3 - 6% in absolute accuracy across all tasks, demonstrating a significant improvement in long-term temporal understanding while maintaining efficiency. Additionally, Koala also improves the model‚Äôs accuracy on short-term action recognition.</p></li></ul></li></ul></details><h3 id="MA-LMM-Memory-Augmented-Large-Multimodal-Model-for-Long-Term-Video-Understanding"><a href="#MA-LMM-Memory-Augmented-Large-Multimodal-Model-for-Long-Term-Video-Understanding" class="headerlink" title="MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding"></a>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</h3><ul><li><code>Keypoints:</code>Memory bank; Long-Term Video Understanding;</li><li><code>Objective:</code>Instead of trying to process more frames simultaneously like most existing work, they propose to process videos in an online manner and store past video information in a memory bank.</li><li><details><summary>Details</summary><ul><li><code>Method:</code><ul><li>Auto-regressively process video frames in an online manner;</li><li>Visual Feature ExtractionÔºöwe inject temporal ordering information into the frame-level features by a position embedding layer (P E);</li><li>Long-term Temporal ModelingÔºö(1) cross-attention layer, which interacts with the raw visual embedding extracted from the frozen visual encoder, and (2) self-attention layer, which models interactions within the input queries.</li><li>Memory Bank CompressionÔºö we simply average the selected token features at all the spatial locations to reduce the memory bank length by 1Ôºõ</li></ul></li></ul></li></ul></details><h3 id="MovieChat-From-Dense-Token-to-Sparse-Memory-for-Long-Video-Understanding"><a href="#MovieChat-From-Dense-Token-to-Sparse-Memory-for-Long-Video-Understanding" class="headerlink" title="MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"></a>MovieChat: From Dense Token to Sparse Memory for Long Video Understanding</h3><ul><li><p><code>Keypoints:</code>Long video understanding; redundancy of visual tokens; Memory mechanism;</p></li><li><p><code>Objective:</code>For long videos understanding, thery overcome these challenges: computation complexity, memory cost, and long-term temporal connection  by reducing the redundancy of visual tokens in the video and building a memory mechanism to pass the information among a large temporal range.</p></li><li><details><summary>Details</summary><ul><li><p><code>Method:</code>Taking advantage of the AtkinsonShiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with their specially designed memory mechanism:</p><ul><li>short term memory: extracted visual features by sliding window G times without further processing are used to construct short-term memory; The update strategy for short-term memory is based on the First-in-First-out (FIFO) queue;</li><li>long term memory: dense tokens to the sparse memories by merging the most similar tokens in the adjacent frames following ToMe periodically; The goal is to keep RL frames after every merge operation, which also embeds rich information stored in the long-term memory.</li></ul></li><li><p><code>Metric:</code>To enhance the robustness of the results, we simultaneously employ GPT-3.5 and Claude as LLM assistants, with the additional support of human blind rating; MovieChat reads more video frames. In both global mode and breakpoint mode, our method maintains a performance gain in terms of the average accuracy and score provided by LLM assistants and human blind rating;</p></li></ul></li></ul></details><h3 id="Megalodon-Efficient-LLM-Pretraining-and-Inference-with-Unlimited-Context-Length"><a href="#Megalodon-Efficient-LLM-Pretraining-and-Inference-with-Unlimited-Context-Length" class="headerlink" title="Megalodon Efficient LLM Pretraining and Inference with Unlimited Context Length"></a>Megalodon Efficient LLM Pretraining and Inference with Unlimited Context Length</h3><ul><li><code>Keypoints:</code> Exponential moving average with gated attention; LLMÔºõUnlimited Context Length</li><li><code>Objective:</code>We introduce MEGALODON, an neural architecture for efficient sequence modeling with unlimited context length.</li><li><details><summary>Details</summary><ul><li><code>Method:</code>MEGALODON inherits the architecture of MEGA (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration.</li></ul></li></ul></details> <h3 id="TriForce-Lossless-Acceleration-of-Long-Sequence-Generation-with-Hierarchical-Speculative-Decoding"><a href="#TriForce-Lossless-Acceleration-of-Long-Sequence-Generation-with-Hierarchical-Speculative-Decoding" class="headerlink" title="TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding"></a>TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding</h3><ul><li><code>Keypoints:</code> Hierarchical Speculative Decoding„ÄÅLossless Acceleration„ÄÅEfficient Long-Sequence Inference</li><li><code>Objective:</code> The primary objective of the paper is to develop a system that can efficiently accelerate the inference process for long sequence generation in large language models without degrading the quality of the output.</li><li><details><summary>Details</summary><ul><li><code>Method:</code> <ul><li>The TriForce system is presented, which employs a hierarchical speculative decoding approach.</li><li>It leverages a draft model with a partial key-value (KV) cache to generate tokens, which are then verified by a target model using a full KV cache.</li><li>The system uses a lightweight model for initial speculations and a retrieval-based drafting method for selecting relevant KV cache chunks.</li><li>The hierarchical structure allows for addressing the bottlenecks in both model weights and KV cache, leading to improved inference speed.</li></ul></li><li><code>Summary:</code> <ul><li>TriForce achieves significant speedups in long sequence generation, such as up to 2.31x for the Llama2-7B-128K model on the A100 GPU and 7.78x on two RTX 4090 GPUs.  </details></li></ul></li></ul></li></ul><h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><h3 id="iVideoGPT-Interactive-VideoGPTs-are-Scalable-World-Models"><a href="#iVideoGPT-Interactive-VideoGPTs-are-Scalable-World-Models" class="headerlink" title="iVideoGPT: Interactive VideoGPTs are Scalable World Models"></a>iVideoGPT: Interactive VideoGPTs are Scalable World Models</h3><ul><li><code>Keypoints:</code> Video Generation Model; Interactive Video Prediction; VQ-GAN;</li><li><code>Objective:</code> To explore the development of world models that are both interactive and scalable within a GPT-like autoregressive transformer framework, and facilitate interactive behaviour learning.</li></ul><h3 id="Beyond-Scaling-Laws-Understanding-Transformer-Performance-with-Associative-Memory"><a href="#Beyond-Scaling-Laws-Understanding-Transformer-Performance-with-Associative-Memory" class="headerlink" title="Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory"></a>Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory</h3><ul><li><p><code>Keypoints:</code> LLM„ÄÅScaling Laws„ÄÅmemorization;</p></li><li><p><code>Objective:</code> We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models.</p></li><li><details><summary>Details</summary><ul><li><code>Method:</code> We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search. Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism.</li></ul></li></ul></details><h3 id="Diffscaler-Enhancing-the-Generative-Prowess-of-Diffusion-Transformers"><a href="#Diffscaler-Enhancing-the-Generative-Prowess-of-Diffusion-Transformers" class="headerlink" title="Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers"></a>Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers</h3><ul><li><code>Keypoints:</code> Parameter efficient finetuning; Incremental learning for diffusion;</li><li><code>Objective:</code> This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model.</li><li><details><summary>Details</summary><ul><li><code>Method:</code><ul><li>init class embeddings of a new dataset by uc embedding; </li><li>propose a Affiner module to finetune new classes; </li><li>scaling up Affiner to support multiple datasets; </li><li>Diffscaler can serve as an alternative to ControlNet. It does not use a separate network to encode conditions but directly utilizes an Affiner. However, there are no specific descriptions provided. In terms of parameters, Diffscaler adds 7M, whereas ControlNet adds 300M.</li></ul></li></ul></li></ul></details><h3 id="Stacking-Your-Transformers-A-Closer-Look-at-Model-Growth-for-Efficient-LLM-Pre-Training"><a href="#Stacking-Your-Transformers-A-Closer-Look-at-Model-Growth-for-Efficient-LLM-Pre-Training" class="headerlink" title="Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training"></a>Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training</h3><ul><li><code>Keypoints:</code> Model Growth;  LLM Pre-Training;</li><li><code>Objective:</code> To overcome these limitations, we first summarize existing works into four atomic growth operators to represent these growth techniques. Then we build a standardized LLMs training testbed to pre-train LLMs with four growth operators on depthwise and widthwise directions and evaluate the results with both training loss and eight evaluation metrics in Harness.</li></ul><h2 id="camera-control"><a href="#camera-control" class="headerlink" title="camera control"></a>camera control</h2><h3 id="VD3D-Taming-Large-Video-Diffusion-Transformers-for-3D-Camera-Control"><a href="#VD3D-Taming-Large-Video-Diffusion-Transformers-for-3D-Camera-Control" class="headerlink" title="VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control"></a>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</h3><ul><li><p><code>Keypoints:</code>  Transformer-based video diffusion models; camera control motion; video generation; controlnet</p></li><li><p><code>Objective:</code>large video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Plucker coordinates</p></li><li><p><code>Motivation:</code>any attempt to alter the temporal dynamics (such as camera motion) influences spatial communication between the tokens, leading to unnecessary signal propagation and overfitting during the fine-tuning stage</p></li><li><p><code>Method:</code></p><img width="508" alt="qsvX_03PkE78KOQ17PWzh6B-eVY9b4GBR1-wTFtPBE4" src="https://github.com/user-attachments/assets/18806876-2b19-4d3d-9896-26f82426d57d"></li><li><details><summary>Details</summary><ul><li><code>Method:</code><br>based on SnapVideo:<br>video_patch‚Äì&gt;conditioned with camera plucker‚Äì&gt;CrossAttn with textcond‚Äì&gt;FIT Block‚Äì&gt;new latent‚Äì&gt;denoise‚Äì&gt;output:pixel_level_video</li></ul><p>conditioned with camera plucker: similar to controlNet</p></li></ul></details><h2 id="Performance-Others"><a href="#Performance-Others" class="headerlink" title="Performance &amp; Others"></a>Performance &amp; Others</h2><h3 id="What-matters-when-building-vision-language-models"><a href="#What-matters-when-building-vision-language-models" class="headerlink" title="What matters when building vision-language models?"></a>What matters when building vision-language models?</h3><ul><li><p><code>Keypoints:</code> Vision-Language Models (VLMs);Design Decisions; Performance Improvement;</p></li><li><p><code>Objective:</code>The article aims to identify critical factors that influence the performance of vision-language models (VLMs) and to challenge the conventional design choices made in the literature without proper justification. The goal is to make progress in the field by determining which decisions genuinely improve model performance.</p></li><li><details><summary>Details</summary><ul><li><p><code>Method:</code>The researchers conducted extensive experiments on various aspects, including pre-trained models, architectural choices, data selection, and training methodologies. They developed Idefics2, an 8 billion parameter foundational VLM, which was tested and compared with other models. They also explored different design choices such as model architecture, connector modules, multimodal training procedures, and inference efficiency.</p></li><li><p><code>Metric:</code> They achieved state-of-the-art performance within its size category across multiple benchmarks, often matching or exceeding the performance of models four times its size. The model demonstrated efficiency at inference and was released alongside the datasets used for its training, providing a resource for the VLM community. The performance was measured using various multimodal benchmarks like VQAv2, TextVQA, OKVQA, and COCO.</p></li><li><p>Finding 1. For a fixed number of parameters, the quality of the language model backbone has a higher impact on the performance of the final VLM than the quality of the vision backbone</p></li><li><p>Finding 2. The cross-attention architecture performs better than the fully autoregressive one when unimodal pre-trained backbones are kept frozen. However, when training the unimodal backbones, the fully autoregressive architecture outperforms the cross-attention one, even though the latter has more parameters.</p></li><li><p>Finding 3. Unfreezing the pre-trained backbones under the fully autoregressive architecture can lead to training divergences. Leveraging LoRA still adds expressivity to the training and stabilizes it.</p></li><li><p>Finding 4. Reducing the number of visual tokens with learned pooling significantly improves compute efficiency at training and inference while improving performance on downstream tasks.</p></li><li><p>Finding 5. Adapting a vision encoder pre-trained on fixed-size square images to preserve images‚Äô original aspect ratio and resolution does not degrade performance while speeding up training and inference and reducing memory.</p></li><li><p>Finding 6. Splitting images into sub-images during training allow trading compute efficiency for more performance during inference. The increase in performance is particularly noticeable in tasks involving reading text in an image.</p></li></ul></li></ul></details><h3 id="Not-All-Language-Model-Features-Are-Linear"><a href="#Not-All-Language-Model-Features-Are-Linear" class="headerlink" title="Not All Language Model Features Are Linear"></a>Not All Language Model Features Are Linear</h3><ul><li><p><code>Keypoints:</code> Multi-dimensional Features; Language Model Interpretability; Sparse Autoencoders (SAEs);</p></li><li><p><code>Objective:</code>The research aims to challenge the linear representation hypothesis by exploring the presence of inherently multi-dimensional features within language models. The goal is to understand if language models use these multi-dimensional representations for computation and to uncover the underlying algorithms.</p></li><li><details><summary>Details</summary><ul><li><p><code>Method:</code> The authors develop a method using sparse autoencoders to automatically discover multi-dimensional features within GPT-2 and Mistral 7B language models. They propose a superposition hypothesis that accounts for these new features and test for irreducible features using a theoretically grounded and empirically practical test.</p></li><li><p><code>Metric:</code> The effectiveness of the discovered features is validated through intervention experiments on Mistral 7B and Llama 3 8B models, demonstrating that circular features are causally implicated in computing tasks involving modular arithmetic of days and months. The models‚Äô performance on these tasks is measured by comparing the highest logit valid token against the ground truth answer, showing a significant enhancement in understanding the models‚Äô internal representations.</p></li></ul></li></ul></details><h3 id="LLM2Vec-Large-Language-Models-Are-Secretly-Powerful-Text-Encoders"><a href="#LLM2Vec-Large-Language-Models-Are-Secretly-Powerful-Text-Encoders" class="headerlink" title="LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"></a>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</h3><ul><li><code>Keypoints:</code> LLM2Vec; Text Embedding; Unsupervised Learning; Performance Enhancement;</li><li><code>Objective:</code> The research aims to transform decoder-only Large Language Models (LLMs) into powerful text encoders for various NLP tasks without requiring labeled data, thus improving the state-of-the-art in text embedding.</li><li><details><summary>Details</summary><ul><li><p><code>Method:</code>The LLM2Vec approach involves three steps: enabling bidirectional attention, masked next token prediction (MNTP), and unsupervised contrastive learning (SimCSE). This method is applied to LLMs with parameters ranging from 1.3B to 7B.</p></li><li><p><code>Metric:</code>The effectiveness of LLM2Vec is evaluated on word-level tasks (chunking, NER, POS tagging) and sequence-level tasks using the Massive Text Embeddings Benchmark (MTEB). The models demonstrate significant performance improvements, with the best model achieving a score of 56.8 on MTEB, outperforming encoder-only models and setting a new unsupervised state-of-the-art.</p></li></ul></li></ul></details><h3 id="Adapting-LLaMA-Decoder-to-Vision-Transformer"><a href="#Adapting-LLaMA-Decoder-to-Vision-Transformer" class="headerlink" title="Adapting LLaMA Decoder to Vision Transformer"></a>Adapting LLaMA Decoder to Vision Transformer</h3><ul><li><code>Keypoints:</code> LLaMA Decoder Adaptation;Causal Self-Attention; Soft Mask Strategy;</li><li><code>Objective:</code>The research aims to adapt the LLaMA decoder-only architecture, originally designed for large language models, to the field of computer vision. The goal is to explore the potential of using this architecture for tasks such as image classification and to achieve competitive performance compared to encoder-only counterparts.</li><li><details><summary>Details</summary><ul><li><code>Method:</code>The study introduces a series of modifications to align the standard ViT architecture with that of LLaMA. Key modifications include:<ul><li>Repositioning the class token behind image tokens using a post-sequence class token technique to address the attention collapse issue.</li><li>Implementing a soft mask strategy that gradually introduces a causal mask to the self-attention, facilitating optimization.</li><li>Employing causal self-attention to enhance computational efficiency and learn complex representations.</li></ul></li><li><code>Metric:</code>The tailored model, iLLaMA, was evaluated on the ImageNet-1K dataset, achieving a top-1 accuracy of 75.1% with 5.7M parameters. When scaled up and pre-trained on ImageNet-21K, the model further enhanced its accuracy to 86.0%. Extensive experiments demonstrated iLLaMA‚Äôs reliable properties, including calibration, shape-texture bias, quantization compatibility, ADE20K segmentation, and CIFAR transfer learning, rivaling the performance of encoder-only models.</details></li></ul></li></ul><h3 id="RHO-1-Not-All-Tokens-Are-What-You-Need"><a href="#RHO-1-Not-All-Tokens-Are-What-You-Need" class="headerlink" title="RHO-1: Not All Tokens Are What You Need"></a>RHO-1: Not All Tokens Are What You Need</h3><ul><li><code>Keypoints:</code> Reference language modelÔºõ Hard tokens;</li><li><code>Objective:</code> Applying the same loss to all tokens can result in wasted computation on non-beneficial tokens, possibly limiting LLM‚Äôs potential to merely mediocre intelligence.</li><li><details><summary>Details</summary><ul><li><code>Method:</code> <ul><li>Our findings reveal that significant loss reduction is limited to a select group of tokens during training. Many tokens are ‚Äúeasy tokens‚Äù that are already learned, and some are ‚Äúhard tokens‚Äù that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updatesÔºõ</li><li>First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss. Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications.</li></ul></li><li><code>Metric:</code>SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.</li></ul></li></ul></details>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Video Generation</title>
    <link href="/2024/12/23/Video%20Generation/"/>
    <url>/2024/12/23/Video%20Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="Video-Generation"><a href="#Video-Generation" class="headerlink" title="Video Generation"></a>Video Generation</h2><ul><li><p>üìå<a href="#LongVideoGenaration">Long Video Genaration</a></p><ul><li><a href="#moviedreamer-hierarchical-generation-for-coherent-long-visual-sequences">MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences</a><code>[LLM]</code> <code>[2024.07]</code> [<a href="https://arxiv.org/abs/2407.16655">paper</a>] [<a href="https://aim-uofa.github.io/MovieDreamer/">code</a>]</li><li><a href="#storydiffusion-consistent-self-attention-for-long-range-image-and-video-generation">StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</a><code>[SD]</code> <code>[2024.05]</code>[<a href="https://arxiv.org/abs/2405.01434">paper</a>] [<a href="https://github.com/HVision-NKU/StoryDiffusion">code</a>]</li><li><a href="#mofa-video-controllable-image-animation-via-generative-motion-field-adaptions-in-frozen-image-to-video-diffusion-model">MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model</a><code>[SD]</code> <code>[2024.05]</code>[<a href="https://arxiv.org/abs/2405.20222">paper</a>] [<a href="https://github.com/MyNiuuu/MOFA-Video">code</a>]</li><li><a href="#vid-gpt-introducing-gpt-style-autoregressive-generation-in-video-diffusion-models">ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models</a><code>[SD]</code> <code>[2024.06]</code>[<a href="https://arxiv.org/abs/2406.10981">paper</a>] [<a href="https://github.com/Dawn-LX/Causal-VideoGen">code</a>]</li><li><a href="#training-free-long-video-generation-with-chain-of-diffusion-model-experts">Training-free Long Video Generation with Chain of Diffusion Model Experts</a><code>[SD]</code> <code>[2024.08]</code>[<a href="https://arxiv.org/abs/2408.13423">paper</a>] [<a href="https://confiner2025.github.io/">code</a>]</li><li><a href="#cogvideox-text-to-video-diffusion-models-with-an-expert-transformer">CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer</a><code>[SD]</code> <code>[2024.08]</code>[<a href="https://arxiv.org/abs/2408.06072">paper</a>] [<a href="https://github.com/THUDM/CogVideo">code</a>]</li><li><a href="#xgen-videosyn-1-high-fidelity-text-to-video-synthesis-with-compressed-representations">xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations</a><code>[SD]</code> <code>[2024.08]</code>[<a href="https://arxiv.org/abs/2408.12590">paper</a>] </li><li><a href="#od-vae-an-omni-dimensional-video-compressor-for-improving-latent-video-diffusion-model">OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model</a><code>[VAE]</code> <code>[2024.09]</code>[<a href="https://arxiv.org/abs/2409.01199">paper</a>] [<a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">code</a>]</li><li>Progressive Autoregressive Video Diffusion Models <code>[SD+AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.08151">paper</a>]</li><li>Loong: Generating Minute-level Long Videos with Autoregressive Language Models <code>[AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.02757">paper</a>]</li><li>SlowFast-VGenÔºöSlow-Fast Learning for Action-Conditioned Long Video Generation <code>[SD]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2410.23277">paper</a>] [<a href="https://slowfast-vgen.github.io/">Project</a>]</li></ul></li><li><p>üìå<a href="#Controllable-Generation">Controllable Generation</a></p><ul><li><a href="#AC3D-Analyzing-and-Improving-3D-Camera-Control-in-Video-Diffusion-Transformers">AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</a> <code>[DiT]</code> <code>[2024.11]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://snap-research.github.io/ac3d/">project page</a>]</li><li><a href="#MagicDriveDiT-High-Resolution-Long-Video-Generation-for-Autonomous-Driving-with-Adaptive-Control">MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</a> <code>[DiT]</code> <code>[2024.11]</code> <code>[preprint]</code> [<a href="https://arxiv.org/abs/2411.13807">paper</a>][<a href="https://github.com/flymin/MagicDriveDiT">code</a>]</li><li><a href="#cinemo-consistent-and-controllable-image-animation-with-motion-diffusion-models">Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</a> <code>[DiT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</li><li><a href="#vd3d-taming-large-video-diffusion-transformers-for-3d-camera-control">VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</a>    <code>[Snap Video FIT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.12781">paper</a>][<a href="https://snap-research.github.io/vd3d/index.html">code</a>]</li><li><a href="#vivid-zoo-multi-view-video-generation-with-diffusion-model">Vivid-ZOO: Multi-View Video Generation with Diffusion Model</a>    <code>[SD]</code> <code>[2024.6]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2406.08659">paper</a>][<a href="https://github.com/hi-zhengcheng/vividzoo">code</a>]</li><li><a href="#training-free-camera-control-for-video-generation">Training-free Camera Control for Video Generation</a>    <code>[svd]</code> <code>[2024.6]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2406.10126">paper</a>][<a href="https://lifedecoder.github.io/CamTrol/">code</a>]</li><li><a href="#motionclone-training-free-motion-cloning-for-controllable-video-generation">MotionClone: Training-Free Motion Cloning for Controllable Video Generation</a> <code>[SVD]</code> <code>[2024.6]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2406.05338">paper</a>][<a href="https://github.com/Bujiazi/MotionClone">code</a>]</li><li><a href="#controlling-space-and-time-with-diffusion-models">Controlling Space and Time with Diffusion Models</a><code>[DiT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</li><li><a href="#revideo-remake-a-video-with-motion-and-content-control">ReVideo: Remake a Video with Motion and Content Control</a><code>[SVD]</code> <code>[2024.5]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2405.13865">paper</a>][<a href="https://github.com/MC-E/ReVideo">code</a>]</li><li><a href="#motionmaster-training-free-camera-motion-transfer-for-video-generation">MotionMaster: Training-free Camera Motion Transfer For Video Generation</a><code>[diffusion]</code> <code>[2024.5]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2404.15789">paper</a>][]</li><li><a href="#collaborative-video-diffusion-consistent-multi-video-generation-with-camera-control">Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control</a><code>[SVD]</code> <code>[2024.5]</code> <code>[preprint]</code>[<a href="https://collaborativevideodiffusion.github.io/assets/pdfs/paper.pdf">paper</a>][<a href="https://collaborativevideodiffusion.github.io/">code</a>]</li><li><a href="#camvig-camera-aware-image-to-video-generation-with-multimodal-transformers">CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers</a><code>[transformer]</code> <code>[2024.5]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2405.13195">paper</a>][]</li><li><a href="#trackgo-a-flexible-and-efficient-method-for-controllable-video-generation">TrackGo: A Flexible and Efficient Method for Controllable Video Generation</a><code>[SVD]</code> <code>[2024.8]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2408.11475">paper</a>][<a href="https://zhtjtcz.github.io/TrackGo-Page/#">code</a>]</li><li><a href="#sv3d-novel-multi-view-synthesis-and-3d-generation-from-a-single-image-using-latent-video-diffusion">SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</a><code>[SVD]</code> <code>[2024.3]</code> <code>[ECCV24]</code>[<a href="https://arxiv.org/pdf/2403.12008">paper</a>][<a href="https://github.com/Stability-AI/generative-models/tree/sv3d_gradio?tab=readme-ov-file">code</a>]</li><li><a href="#DREAMVIDEO-2-ZERO-SHOT-SUBJECT-DRIVEN-VIDEO-CUSTOMIZATION-WITH-PRECISE-MOTION-CONTROL">DREAMVIDEO-2: ZERO-SHOT SUBJECT-DRIVEN VIDEO CUSTOMIZATION WITH PRECISE MOTION CONTROL</a><code>[SVD]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/pdf/2403.12008">paper</a>][<a href="https://dreamvideo2.github.io/">project page</a>]</li><li>Trajectory Attention for Fine-grained Video Motion Control]<code>[SVD]</code> <code>[2024.11]</code> <code>[ICLR25]</code> [<a href="https://arxiv.org/abs/2411.19324">paper</a>][<a href="https://xizaoqu.github.io/trajattn/">project page</a>]</li></ul></li><li><p>üìå<a href="#HighFidelty">High Fidelty</a></p><ul><li><a href="#Factorized-Dreamer-Training-A-High-Quality-Video-Generator-with-Limited-and-Low-Quality-Data">Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data</a><code>[2024.07]</code>[<a href="https://arxiv.org/abs/2408.13252">paper</a>]</li><li><a href="#DisCo-Diff-Enhancing-Continuous-Diffusion-Models-with-Discrete-Latents">DisCo-Diff: Enhancing Continuous Dif969272fusion Models with Discrete Latents</a><code>[2024.07]</code>[<a href="https://arxiv.org/abs/2407.03300">paper</a>]</li><li><a href="#No-Training-No-Problem-Rethinking-Classifier-Free-Guidance-for-Diffusion-Models">No-Training-No-Problem-Rethinking-Classifier-Free-Guidance-for-Diffusion-Models</a><code>[2024.07]</code>[<a href="https://arxiv.org/abs/2407.02687">paper</a>]</li><li><a href="#VEnhancer-Generative-Space-Time-Enhancement-for-Video-Generation">VEnhancer: Generative Space-Time Enhancement for Video Generation</a><code>[2024.07]</code>[<a href="https://arxiv.org/abs/2407.07667">paper</a>]</li><li><a href="#Vista-A-Generalizable-Driving-World-Model-with-High-Fidelity-and-Versatile-Controllability">Vista-A-Generalizable-Driving-World-Model-with-High-Fidelity-and-Versatile-Controllability</a><code>[2024.07]</code>[<a href="https://arxiv.org/abs/2405.17398">paper</a>]</li><li>Pyramidal Flow Matching for Efficient Video Generative Modeling <code>[2024.10]</code>[<a href="https://arxiv.org/abs/2410.05954">paper</a>] [<a href="https://github.com/jy0205/Pyramid-Flow">code</a>]</li><li>Movie Gen: A cast of Media Foundation Models <code>[2024.10]</code>[<a href="https://ai.meta.com/static-resource/movie-gen-research-paper">paper</a>]</li></ul></li><li><p>üìå<a href="#Efficiency">Efficiency</a></p><ul><li><a href="#efficient-video-diffusion-models-via-content-frame-motion-latent-decomposition">Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition</a> <code>[2024.05]</code> <code>[ICLR24]</code> [<a href="https://arxiv.org/abs/2403.14148">paper</a>]</li><li><a href="#Efficient-Conditional-Diffusion-Model-with-Probability-Flow-Sampling-for-Image-Super-resolution">Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution</a> <code>[2024.04]</code> <code>[AAAI24]</code> [<a href="https://arxiv.org/abs/2404.10688">paper</a>] [<a href="https://github.com/Yuan-Yutao/ECDP">code</a>]</li><li><a href="#T2V-Turbo-Breaking-the-Quality-Bottleneck-of-Video-Consistency-Model-with-Mixed-Reward-Feedback">T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</a> <code>[2024.05]</code> [<a href="https://arxiv.org/abs/2405.18750">paper</a>] [<a href="https://github.com/Ji4chenLi/t2v-turbo">code</a>]</li><li><a href="#Phased-Consistency-Model">Phased Consistency Model</a> <code>[2024.05]</code> [<a href="https://arxiv.org/abs/2405.18407">paper</a>] [<a href="https://github.com/G-U-N/Phased-Consistency-Model/tree/master">code</a>]</li><li><a href="#EM-Distillation-for-One-step-Diffusion-Models">EM Distillation for One-step Diffusion Models</a> <code>[2024.05]</code> [<a href="https://arxiv.org/abs/2405.16852">paper</a>]</li><li><a href="#Large-Kernel-Distillation-Network-for-Efficient-Single-Image-Super-Resolution">Large Kernel Distillation Network for Efficient Single Image Super-Resolution</a> <code>[2024.07]</code> <code>[CVPRW 2023]</code> [<a href="https://arxiv.org/abs/2407.14340">paper</a>] [<a href="https://github.com/stella-von/LKDN">code</a>]</li><li><a href="#One-Step-Effective-Diffusion-Network-for-Real-World-Image-Super-Resolution">One-Step Effective Diffusion Network for Real-World Image Super-Resolution</a> <code>[2024.06]</code> [<a href="https://arxiv.org/abs/2406.08177">paper</a>] [<a href="https://github.com/cswry/OSEDiff">code</a>]</li><li>Adaptive Caching for Faster Video Generation with Diffusion Transformers <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.02397">paper</a>] [<a href="https://github.com/AdaCache-DiT/AdaCache">code</a>]</li><li>REDUCIO! Generating 1024√ó1024 Video within 16 Seconds using Extremely Compressed Motion Latents <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.13552">paper</a>] [<a href="https://github.com/microsoft/Reducio-VAE">code</a>]</li><li>Timestep Embedding Tells: It‚Äôs Time to Cache for Video Diffusion Model <code>[2024.11]</code> [<a href="https://www.arxiv.org/abs/2411.19108">paper</a>] [<a href="https://github.com/LiewFeng/TeaCache">code</a>]</li></ul></li><li><p>üìå<a href="#Multiview">Multiview</a></p><ul><li><a href="#customizing-text-to-image-diffusion-with-camera-viewpoint-control">Customizing Text-to-Image Diffusion with Camera Viewpoint Control</a><code>[2024.4]</code> <code>[preprint]</code> [<a href="https://arxiv.org/abs/2404.12333">paper</a>][<a href="https://github.com/customdiffusion360/custom-diffusion360">code</a>]</li><li><a href="#layerpano3d-layered-3d-panorama-for-hyper-immersive-scene-generation">LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation</a><code>[2024.8]</code> <code>[preprint]</code> [<a href="https://arxiv.org/abs/2408.13252">paper</a>][<a href="https://ys-imtech.github.io/projects/LayerPano3D/">code</a>]</li><li>[From Bird‚Äôs-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model]</li><li>[DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes]</li><li>[SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control]</li><li>[FreeVS: Generative View Synthesis on Free Driving Trajectory]<code>[2024.10]</code> [<a href="https://arxiv.org/pdf/2410.18079">paper</a>] [<a href="https://freevs24.github.io/">code</a>]</li></ul></li></ul><h3 id="üìåReconstruction"><a href="#üìåReconstruction" class="headerlink" title="üìåReconstruction"></a>üìåReconstruction</h3><ul><li>DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation <code>[4DGS]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.13571">paper</a>] <code>DriveDreamer4D leverages world models priors to generate diverse viewpoint data, enhancing the 4D scene representation</code></li></ul><h3 id="üìåEnd2End-Autonomous-Driving"><a href="#üìåEnd2End-Autonomous-Driving" class="headerlink" title="üìåEnd2End Autonomous Driving"></a>üìåEnd2End Autonomous Driving</h3><ul><li>DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving  <code>[2024.12]</code> [<a href="https://arxiv.org/pdf/2411.15139">paper</a>] [<a href="https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du----duan-dao-duan-zi-dong-jia-shi-%E3%80%91DiffusionDrive-%20Truncated%20Diffusion%20Model%20for%20End-to-End%20Autonomous%20Driving.html">paper_analysis</a>]  <code>They propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution</code></li></ul>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Video Generation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image Generation</title>
    <link href="/2024/12/23/Image%20Generation/"/>
    <url>/2024/12/23/Image%20Generation/</url>
    
    <content type="html"><![CDATA[<h1 id="Image-Generation"><a href="#Image-Generation" class="headerlink" title="Image Generation"></a>Image Generation</h1><h3 id="üìåControllable-Generation"><a href="#üìåControllable-Generation" class="headerlink" title="üìåControllable Generation"></a>üìåControllable Generation</h3><ul><li><a href="#MAGICTAILOR-COMPONENT-CONTROLLABLE-PERSONALIZATION-IN-TEXT-TO-IMAGE-DIFFUSION-MODELS">MAGICTAILOR:COMPONENT-CONTROLLABLE PERSONALIZATION IN TEXT-TO-IMAGE DIFFUSION MODELS</a> <code>[DiT]</code> <code>[2024.07]</code> <code>[preprint]</code> [<a href="https://correr-zhou.github.io/MagicTailor/">project page</a>][<a href="https://github.com/correr-zhou/MagicTailor">code</a>]\</li><li>ControlNeXt: Powerful and Efficient Control for Image and Video Generation<code>[SD]</code> <code>[2024.08]</code> [<a href="https://arxiv.org/abs/2408.06070">paper</a>] [<a href="https://github.com/dvlab-research/ControlNeXt">code</a>]</li><li>Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models <code>[SD]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.02416">paper</a>]</li><li>Improving Long-Text Alignment for Text-to-Image Diffusion Models<code>[SD]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.11817">paper</a>] [<a href="https://github.com/luping-liu/LongAlign">code</a>]</li><li>Diversity-Rewarded CFG Distillation <code>[2024.08]</code> [<a href="https://arxiv.org/abs/2410.06084">paper</a>]</li><li>InstanceDiffusion:Instance-level Control for Image Generation<code>[SD]</code> <code>[2024.02]</code> [<a href="https://arxiv.org/abs/2402.03290">paper</a>] [<a href="https://github.com/frank-xwang/InstanceDiffusion">code</a>]</li><li>IFAdapter: Instance feature control for grounded Text-to-Image Generation <code>[SD]</code> <code>[2024.09]</code> [<a href="https://arxiv.org/abs/2409.08240">paper</a>]</li><li>Taming Rectified Flow for Inversion and Editing <code>[DIT]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.04746">paper</a>] [<a href="https://github.com/wangjiangshan0725/RF-Solver-Edit">code</a>]</li><li>OminiControl: Minimal and Universal Control for Diffusion Transformer <code>[DIT]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.15089">paper</a>] [<a href="https://github.com/Yuanshi9815/OminiControl">code</a>]</li><li>ROICtrl: Boosting Instance Control for Visual Generation <code>[SD]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.17949">paper</a>] [<a href="https://github.com/showlab/ROICtrl">code</a>]</li><li>Diffusion Self-Distillation for Zero-Shot Customized Image Generation <code>[DIT]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.18616">paper</a>]</li><li>On Inductive Biases That Enable Generalization of Diffusion Transformers <code>[DIT]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.21273">paper</a>] [<a href="https://github.com/DiT-Generalization/DiT-Generalization">code</a>]</li></ul><h3 id="üìåSuper-Resolution"><a href="#üìåSuper-Resolution" class="headerlink" title="üìåSuper Resolution"></a>üìåSuper Resolution</h3><ul><li>MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning<code>[SD]</code> <code>[2024.08]</code> [<a href="https://arxiv.org/abs/2408.11001">paper</a>] [<a href="https://haoningwu3639.github.io/MegaFusion">code</a>]</li></ul><h3 id="üìåMultiModal"><a href="#üìåMultiModal" class="headerlink" title="üìåMultiModal"></a>üìåMultiModal</h3><ul><li>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation <code>[AR]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.07975">paper</a>] [<a href="">paper analysis</a>]</li><li>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation <code>[AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.13848">paper</a>] [<a href="https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html">paper analysis</a>]</li><li>MONOFORMER: ONE TRANSFORMER FOR BOTH DIFFUSION AND AUTOREGRESSION <code>[SD+AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2409.16280">paper</a>] [<a href="https://github.com/MonoFormer/MonoFormer">code</a>] [<a href="https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html">paper analysis</a>]</li><li>SHOW-O: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION <code>[SD+AR]</code> <code>[2024.08]</code> [<a href="https://arxiv.org/abs/2408.12528">paper</a>] [<a href="https://github.com/showlab/Show-o">code</a>] [<a href="https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html">paper analysis</a>]</li><li>Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models<code>[DiT]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/abs/2411.04996">paper</a>] [<a href="https://github.com/sanowl/Mixture-of-Transformers-A-Sparse-and-Scalable-Architecture-for-Multi-Modal-Foundation-Model-meta">code</a>]</li></ul><h3 id="üìåAutoregressive-Generation"><a href="#üìåAutoregressive-Generation" class="headerlink" title="üìåAutoregressive Generation"></a>üìåAutoregressive Generation</h3><ul><li>DART: DENOISING AUTOREGRESSIVE TRANSFORMER FOR SCALABLE TEXT-TO-IMAGE GENERATION <code>[SD+AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.08159">paper</a>]</li><li>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens <code>[AR]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/pdf/2410.13863">paper</a>]</li></ul><h3 id="üìåEfficient"><a href="#üìåEfficient" class="headerlink" title="üìåEfficient"></a>üìåEfficient</h3><ul><li>Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think <code>[DiT]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.06940">paper</a>]</li><li>SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers <code>[DiT]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.10629">paper</a>]</li><li>RECTIFIED DIFFUSION: STRAIGHTNESS IS NOT YOUR NEED IN RECTIFIED FLOW <code>[SD]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/pdf/2410.07303">paper</a>]</li><li>Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models <code>[CM]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/abs/2410.11081">paper</a>]</li></ul><h3 id="üìåDataset-Expansion"><a href="#üìåDataset-Expansion" class="headerlink" title="üìåDataset Expansion"></a>üìåDataset Expansion</h3><ul><li>Scalable Ranked Preference Optimization for Text-to-Image Generation <code>[SD]</code> <code>[2024.10]</code> [<a href="https://arxiv.org/pdf/2410.18013">paper</a>]</li></ul><h3 id="üìåHigh-Fidelity-Generation"><a href="#üìåHigh-Fidelity-Generation" class="headerlink" title="üìåHigh-Fidelity Generation"></a>üìåHigh-Fidelity Generation</h3><ul><li>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models <code>[SD]</code> <code>[2024.11]</code> [<a href="https://arxiv.org/pdf/2411.07126">paper</a>]</li></ul>]]></content>
    
    
    <categories>
      
      <category>Image Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Image Generation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scaling Law</title>
    <link href="/2024/12/23/Scaling_Law/"/>
    <url>/2024/12/23/Scaling_Law/</url>
    
    <content type="html"><![CDATA[<h2 id="Scaling-Law"><a href="#Scaling-Law" class="headerlink" title="Scaling Law"></a>Scaling Law</h2><ol><li>SCALING LAWS FOR DIFFUSION TRANSFORMERS </li><li>Towards Precise Scaling Laws for Video Diffusion Transformers Âø´Êâã</li><li>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens:</li></ol><p>‚ÄãÂü∫‰∫éËøûÁª≠tokenÁöÑÊ®°ÂûãÊØîÁ¶ªÊï£tokenÊ®°ÂûãÂú®ËßÜËßâË¥®Èáè‰∏äÊõ¥Â•Ω„ÄÇ</p><p>‚ÄãÈöèÊú∫È°∫Â∫èÁîüÊàê‰∏éÂÖâÊ†ÖÈ°∫Â∫èÁõ∏ÊØîÂú®GenEvalÊµãËØï‰∏äÂæóÂàÜÊòéÊòæÊõ¥Â•Ω</p>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Scaling Law</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DiT in Video Generation</title>
    <link href="/2024/12/23/DiT_in_Video_Generation/"/>
    <url>/2024/12/23/DiT_in_Video_Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="DiT-in-Video-Generation"><a href="#DiT-in-Video-Generation" class="headerlink" title="DiT in Video Generation"></a>DiT in Video Generation</h2><h3 id="DiTÁöÑÈóÆÈ¢ò"><a href="#DiTÁöÑÈóÆÈ¢ò" class="headerlink" title="DiTÁöÑÈóÆÈ¢ò"></a>DiTÁöÑÈóÆÈ¢ò</h3><ul><li><p>Êó∂Â∫è‰∏ÄËá¥ÊÄß</p></li><li><p>Êó∂Á©∫ÁºñÁ†Å</p></li><li><p>DiTÁöÑÂèÇÊï∞Âà©Áî®Áéá‰Ωé</p></li></ul><h3 id="Diffusion"><a href="#Diffusion" class="headerlink" title="Diffusion"></a>Diffusion</h3><p>1.Pyramidal Flow Matching for Efficient Video Generative Modeling </p><p>2.Mimir: Improving Video Diffusion Models for Precise Text Understanding</p><p>3.Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</p><p>4.MarDini: Masked Auto-Regressive Diffusion for Video Generation at Scale Meta</p><p>5.DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models</p><p>6.Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing</p><p>7.ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models Huawei Cloud, Ê≤°ÊúâËßÜÈ¢ëÔºå‰∏çÂ§™work</p><h3 id="DiT"><a href="#DiT" class="headerlink" title="DiT"></a>DiT</h3><p>CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer </p><p>Open-Sora Plan: Open-Source Large Video Generation Model</p><p>WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</p><h4 id="Pyramidal-Flow-Matching-for-Efficient-Video-Generative-Modeling"><a href="#Pyramidal-Flow-Matching-for-Efficient-Video-Generative-Modeling" class="headerlink" title="Pyramidal Flow Matching for Efficient Video Generative Modeling"></a>Pyramidal Flow Matching for Efficient Video Generative Modeling</h4><p><strong>keypoints:</strong></p><p>DiTÔºåEfficiencyÔºåvideo generation</p><p><strong>objective:</strong></p><p>the necessity of modeling a significantly large spatiotemporal space makes the training of such video generative models computationally and data intensive.</p><p><img src="/img/image-20241223202107449.png" alt="image-20241223202107449"></p><p><strong>Summary:</strong></p><p>ÈÄöËøáÊó∂Â∫èÂíåÁ©∫Èó¥pyramidalÁªìÊûÑÁõ∏ÁªìÂêàÔºåÂ§ßÂ§ßÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÊó∂ÈúÄË¶ÅÁöÑtokenÊï∞ÈáèÔºåÊèêÂçáËÆ≠ÁªÉÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊÄßËÉΩÁöÑ‰ºòË∂äÊÄß</p>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DiT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AutoRegressive in Video Generation</title>
    <link href="/2024/12/23/AutoRegressive_in_Video_Generation/"/>
    <url>/2024/12/23/AutoRegressive_in_Video_Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="AutoRegressive-in-Video-Generation"><a href="#AutoRegressive-in-Video-Generation" class="headerlink" title="AutoRegressive in Video Generation"></a>AutoRegressive in Video Generation</h2><h3 id="Ëá™ÂõûÂΩíÂ≠òÂú®ÁöÑÈóÆÈ¢òÔºö"><a href="#Ëá™ÂõûÂΩíÂ≠òÂú®ÁöÑÈóÆÈ¢òÔºö" class="headerlink" title="Ëá™ÂõûÂΩíÂ≠òÂú®ÁöÑÈóÆÈ¢òÔºö"></a>Ëá™ÂõûÂΩíÂ≠òÂú®ÁöÑÈóÆÈ¢òÔºö</h3><ul><li><p>VAEÁöÑÁ¶ªÊï£ÂåñÁöÑ‰ø°ÊÅØÊçüÂ§±</p></li><li><p>‰∏ä‰∏ãÊñáÁ™óÂè£ÂØπ‰∫éÂõæÂÉèÊù•ËØ¥ËøáÈïø</p></li><li><p>cascadeÁöÑ‰ø°ÊÅØÊçüËÄó</p></li><li><p>low level &amp; high level featureÁöÑ‰ø°ÊÅØÂØÜÂ∫¶‰∏çÂêå</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Control-Based Video Generation</title>
    <link href="/2024/12/23/Control-Based%20Video%20Generation/"/>
    <url>/2024/12/23/Control-Based%20Video%20Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="Control-Based-Video-Generation"><a href="#Control-Based-Video-Generation" class="headerlink" title="Control-Based Video Generation"></a>Control-Based Video Generation</h2><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul><li><a href="#cinemo-consistent-and-controllable-image-animation-with-motion-diffusion-models">Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</a>    <code>[DiT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</li><li><a href="#vd3d-taming-large-video-diffusion-transformers-for-3d-camera-control">VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</a>    <code>[Snap Video FIT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.12781">paper</a>][<a href="https://snap-research.github.io/vd3d/index.html">code</a>]</li><li><a href="#vivid-zoo-multi-view-video-generation-with-diffusion-model">Vivid-ZOO: Multi-View Video Generation with Diffusion Model</a>    <code>[SD]</code> <code>[2024.6]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2406.08659">paper</a>][<a href="https://github.com/hi-zhengcheng/vividzoo">code</a>]</li><li><a href="#training-free-camera-control-for-video-generation">Training-free Camera Control for Video Generation</a>    <code>[svd]</code> <code>[2024.6]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2406.10126">paper</a>][<a href="https://lifedecoder.github.io/CamTrol/">code</a>]</li><li><a href="#motionclone-training-free-motion-cloning-for-controllable-video-generation">MotionClone: Training-Free Motion Cloning for Controllable Video Generation</a> <code>[SVD]</code> <code>[2024.6]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2406.05338">paper</a>][<a href="https://github.com/Bujiazi/MotionClone">code</a>]</li><li><a href="#controlling-space-and-time-with-diffusion-models">Controlling Space and Time with Diffusion Models</a><code>[DiT]</code> <code>[2024.7]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</li><li><a href="#revideo-remake-a-video-with-motion-and-content-control">ReVideo: Remake a Video with Motion and Content Control</a><code>[SVD]</code> <code>[2024.5]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2405.13865">paper</a>][<a href="https://github.com/MC-E/ReVideo">code</a>]</li><li><a href="#motionmaster-training-free-camera-motion-transfer-for-video-generation">MotionMaster: Training-free Camera Motion Transfer For Video Generation</a><code>[diffusion]</code> <code>[2024.5]</code> <code>[preprint]</code> [<a href="https://arxiv.org/pdf/2404.15789">paper</a>][]</li><li><a href="#collaborative-video-diffusion-consistent-multi-video-generation-with-camera-control">Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control</a><code>[SVD]</code> <code>[2024.5]</code> <code>[preprint]</code>[<a href="https://collaborativevideodiffusion.github.io/assets/pdfs/paper.pdf">paper</a>][<a href="https://collaborativevideodiffusion.github.io/">code</a>]</li><li><a href="#camvig-camera-aware-image-to-video-generation-with-multimodal-transformers">CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers</a><code>[transformer]</code> <code>[2024.5]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2405.13195">paper</a>][]</li><li><a href="#trackgo-a-flexible-and-efficient-method-for-controllable-video-generation">TrackGo: A Flexible and Efficient Method for Controllable Video Generation</a><code>[SVD]</code> <code>[2024.8]</code> <code>[preprint]</code>[<a href="https://arxiv.org/pdf/2408.11475">paper</a>][<a href="https://zhtjtcz.github.io/TrackGo-Page/#">code</a>]</li><li><a href="#sv3d-novel-multi-view-synthesis-and-3d-generation-from-a-single-image-using-latent-video-diffusion">SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</a><code>[SVD]</code> <code>[2024.3]</code> <code>[ECCV24]</code>[<a href="https://arxiv.org/pdf/2403.12008">paper</a>][<a href="https://github.com/Stability-AI/generative-models/tree/sv3d_gradio?tab=readme-ov-file">code</a>]</li></ul><h1 id="Controllable-Generation"><a href="#Controllable-Generation" class="headerlink" title="Controllable Generation"></a>Controllable Generation</h1><h3 id="Cinemo-Consistent-and-Controllable-Image-Animation-with-Motion-Diffusion-Models"><a href="#Cinemo-Consistent-and-Controllable-Image-Animation-with-Motion-Diffusion-Models" class="headerlink" title="Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models"></a>Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</h3><p>[DiT][2024.7][preprint][<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</p><ul><li><code>Keypoints:</code> Consistent and Controllable I2V, Diffusion</li><li><code>Key Takeaways:</code> a simple yet effective model that excels in both image consistency and motion controllability.</li></ul><h3 id="VD3D-Taming-Large-Video-Diffusion-Transformers-for-3D-Camera-Control"><a href="#VD3D-Taming-Large-Video-Diffusion-Transformers-for-3D-Camera-Control" class="headerlink" title="VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control"></a>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</h3><p>[Snap Video FIT][2024.7][preprint][<a href="https://arxiv.org/pdf/2407.12781">paper</a>][<a href="https://snap-research.github.io/vd3d/index.html">code</a>]</p><ul><li><code>Keypoints:</code>  Transformer-based video diffusion models; camera control motion; video generation; controlnet</li><li><code>Key Takeaways:</code>large video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Plucker coordinates</li></ul><h3 id="Vivid-ZOO-Multi-View-Video-Generation-with-Diffusion-Model"><a href="#Vivid-ZOO-Multi-View-Video-Generation-with-Diffusion-Model" class="headerlink" title="Vivid-ZOO: Multi-View Video Generation with Diffusion Model"></a>Vivid-ZOO: Multi-View Video Generation with Diffusion Model</h3><p>[SD][2024.6][preprint][<a href="https://arxiv.org/pdf/2406.08659">paper</a>][<a href="https://github.com/hi-zhengcheng/vividzoo">code</a>]</p><ul><li><code>Keypoints:</code> Multi-View Video Generation, Diffusion</li><li><code>Key Takeaways:</code> alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, new multiview dataset</li></ul><h3 id="Training-free-Camera-Control-for-Video-Generation"><a href="#Training-free-Camera-Control-for-Video-Generation" class="headerlink" title="Training-free Camera Control for Video Generation"></a>Training-free Camera Control for Video Generation</h3><p>[svd][2024.6][preprint][<a href="https://arxiv.org/pdf/2406.10126">paper</a>][<a href="https://lifedecoder.github.io/CamTrol/">code</a>]</p><ul><li><code>Keypoints:</code> Training-free Camera Control ,diffusion,t2v</li><li><code>Key Takeaways:</code> offers camera control for off-the-shelf video diffusion models in a training-free but robust manner offers camera control for off-the-shelf video diffusion models in a training-free but robust manner</li></ul><h3 id="MotionClone-Training-Free-Motion-Cloning-for-Controllable-Video-Generation"><a href="#MotionClone-Training-Free-Motion-Cloning-for-Controllable-Video-Generation" class="headerlink" title="MotionClone: Training-Free Motion Cloning for Controllable Video Generation"></a>MotionClone: Training-Free Motion Cloning for Controllable Video Generation</h3><p>[SVD][2024.6][preprint][<a href="https://arxiv.org/pdf/2406.05338">paper</a>][<a href="https://github.com/Bujiazi/MotionClone">code</a>]</p><ul><li><code>Keypoints:</code></li><li><code>Key Takeaways:</code></li><li><details><summary>Details</summary><ul><li><code>Method:</code></details></li></ul></li></ul><h3 id="Controlling-Space-and-Time-with-Diffusion-Models"><a href="#Controlling-Space-and-Time-with-Diffusion-Models" class="headerlink" title="Controlling Space and Time with Diffusion Models"></a>Controlling Space and Time with Diffusion Models</h3><p>[DiT][2024.7][preprint][<a href="https://arxiv.org/pdf/2407.07860">paper</a>][<a href="https://4d-diffusion.github.io/">code</a>]</p><ul><li><code>Keypoints:</code> 4D novel view synthesisÔºõdiffusion modelÔºõ</li><li><code>Key Takeaways:</code> 4DiM, a pixel-based diffusion model for novel view synthesis conditioned on one or more images of arbitrary scenes, camera pose, and time.</li></ul><h3 id="ReVideo-Remake-a-Video-with-Motion-and-Content-Control"><a href="#ReVideo-Remake-a-Video-with-Motion-and-Content-Control" class="headerlink" title="ReVideo: Remake a Video with Motion and Content Control"></a>ReVideo: Remake a Video with Motion and Content Control</h3><p>[SVD][2024.5][preprint][<a href="https://arxiv.org/pdf/2405.13865">paper</a>][<a href="https://github.com/MC-E/ReVideo">code</a>]</p><ul><li><code>Keypoints:</code> SVD-based Video Editing</li><li><code>Key Takeaways:</code> accurately edit content and motion in specific areas of a video through a single control module</li></ul><h3 id="MotionMaster-Training-free-Camera-Motion-Transfer-For-Video-Generation"><a href="#MotionMaster-Training-free-Camera-Motion-Transfer-For-Video-Generation" class="headerlink" title="MotionMaster: Training-free Camera Motion Transfer For Video Generation"></a>MotionMaster: Training-free Camera Motion Transfer For Video Generation</h3><p>[diffusion][2024.5][preprint][<a href="https://arxiv.org/pdf/2404.15789">paper</a>][]</p><ul><li><code>Keypoints:</code> Video Generation, Video Motion, Camera Motion Extraction, Disentanglement</li><li><code>Key Takeaways:</code> Disentangles camera motions and object motions in source videos, and transfers the extracted camera motions to new videos</li></ul><h3 id="Collaborative-Video-Diffusion-Consistent-Multi-video-Generation-with-Camera-Control"><a href="#Collaborative-Video-Diffusion-Consistent-Multi-video-Generation-with-Camera-Control" class="headerlink" title="Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control"></a>Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control</h3><p>[SVD][2024.5][preprint][<a href="https://collaborativevideodiffusion.github.io/assets/pdfs/paper.pdf">paper</a>][<a href="https://collaborativevideodiffusion.github.io/">code</a>]</p><ul><li><code>Keypoints:</code> multiview video generation</li><li><code>Key Takeaways:</code> generates multi-view consistent videos with camera control &amp; align features across diverse input videos for enhanced consistency</li></ul><h3 id="CamViG-Camera-Aware-Image-to-Video-Generation-with-Multimodal-Transformers"><a href="#CamViG-Camera-Aware-Image-to-Video-Generation-with-Multimodal-Transformers" class="headerlink" title="CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers"></a>CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers</h3><p>[transformer][2024.5][preprint][<a href="https://arxiv.org/pdf/2405.13195">paper</a>][]</p><ul><li><code>Keypoints:</code> transformerÔºõ camerapose tokenizerÔºõ</li><li><code>Key Takeaways:</code> We extend multimodal transformers to include 3D camera motion as a conditioning signal for the task of video generation</li></ul><h3 id="TrackGo-A-Flexible-and-Efficient-Method-for-Controllable-Video-Generation"><a href="#TrackGo-A-Flexible-and-Efficient-Method-for-Controllable-Video-Generation" class="headerlink" title="TrackGo: A Flexible and Efficient Method for Controllable Video Generation"></a>TrackGo: A Flexible and Efficient Method for Controllable Video Generation</h3><p>[SVD][2024.8][preprint][<a href="https://arxiv.org/pdf/2408.11475">paper</a>][<a href="https://zhtjtcz.github.io/TrackGo-Page/#">code</a>]</p><ul><li><code>Keypoints:</code> SVD; Diffusion Video Generation; Motion Control </li><li><code>Key Takeaways:</code> based on motion trajectories caclulate a attention map of motion area for temporal-self-attention</li></ul><h3 id="SV3D-Novel-Multi-view-Synthesis-and-3D-Generation-from-a-Single-Image-using-Latent-Video-Diffusion"><a href="#SV3D-Novel-Multi-view-Synthesis-and-3D-Generation-from-a-Single-Image-using-Latent-Video-Diffusion" class="headerlink" title="SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion"></a>SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</h3><p>[SVD][2024.3][ECCV24][<a href="https://arxiv.org/pdf/2403.12008">paper</a>][<a href="https://github.com/Stability-AI/generative-models/tree/sv3d_gradio?tab=readme-ov-file">code</a>]</p><ul><li><code>Keypoints:</code> 3D Generation, SVD, multi-view image generation</li><li><code>Key Takeaways:</code> camera pose(elevation e and azimuth a angles.): sinusoidal embedding-&gt;MLP-&gt;add to time embedding; combine static orbit(without camera pose cond) and dynamic orbit (with camera pose cond)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Control</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Long-duration Video Generation</title>
    <link href="/2024/12/19/Long-duration%20Video%20Generation/"/>
    <url>/2024/12/19/Long-duration%20Video%20Generation/</url>
    
    <content type="html"><![CDATA[<h2 id="Long-duration-Video-Generation"><a href="#Long-duration-Video-Generation" class="headerlink" title="Long-duration Video Generation"></a>Long-duration Video Generation</h2>]]></content>
    
    
    <categories>
      
      <category>Video Generation</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Reproducing &quot;Slow Thinking Reasoning System&quot; like OpenAI o1</title>
    <link href="/2024/12/18/SlowThinking/"/>
    <url>/2024/12/18/SlowThinking/</url>
    
    <content type="html"><![CDATA[<h2 id="Reproducing-‚ÄúSlow-Thinking-Reasoning-System‚Äù-like-OpenAI-o1"><a href="#Reproducing-‚ÄúSlow-Thinking-Reasoning-System‚Äù-like-OpenAI-o1" class="headerlink" title="Reproducing ‚ÄúSlow Thinking Reasoning System‚Äù like OpenAI o1"></a>Reproducing ‚ÄúSlow Thinking Reasoning System‚Äù like OpenAI o1</h2><h3 id="Slow-Thinking-Reasoning-Definition"><a href="#Slow-Thinking-Reasoning-Definition" class="headerlink" title="Slow Thinking Reasoning Definition:"></a>Slow Thinking Reasoning Definition:</h3><p>a extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions, to tackle challenging tasks</p><h3 id="Technical-Approach"><a href="#Technical-Approach" class="headerlink" title="Technical Approach"></a>Technical Approach</h3><ul><li><p>Solution 1 <strong>RLHF</strong>Ôºö</p><p>a policy model, a reward model, and a search algorithm. During inference, the policy model is guided by the reward model to perform the tree search to find correct solutions to mathematical problems.</p></li><li><p>Solution 2 <strong>SFT</strong>: </p><ul><li><p>Phase 0 <strong>Collect long-form thought dataset</strong>:</p><p>beam search (MCTS) or distill from QwQ-32B-preview &#x2F; DeepSeek-R1-Lite-Preview by multiple rollouts</p></li><li><p>Phase 1 <strong>Instruction Tuning to activate a slow-thinking mode</strong>:</p><p>demonstration data is necessary. This data serves the dual purposes of format adherence (i.e., following a slow-thinking response) and ability elicitation (i.e., activating a slow-thinking mode).</p></li><li><p>Phase 2 <strong>SFT using distilled long-form thought dataset</strong>: </p><p>use distilled long-form thought data to fine-tune the reasoning model, enabling it to invoke a slow-thinking mode. The model is then encouraged to explore challenging problems by generating multiple rollouts, which can result in increasingly more high-quality trajectories that lead to correct answers.</p></li><li><p>Phase 3 <strong>Exploration on hard problems</strong>:</p><p>collect a variety of problems with ground-truth answers for exploration</p><p>challenging problems, which require longer thought processes for reasoning, are particularly useful for improving the model‚Äôs performance</p></li><li><p>Phase 4 <strong>Self-improvement</strong>:</p><ul><li><p><strong>self-improvement:</strong></p><p>as we scale the number of rollouts, we can collect an increasing number of problems, while the quality of the trajectories improves as the reasoning model is further trained. This iterative process is crucial for self-improvement training.</p></li><li><p><strong>optimization:</strong></p><p>use the selected trajectories to further enhance the LLM‚Äôs reasoning abilities.</p><ul><li><strong>SFT</strong></li><li><strong>DPO</strong></li><li><strong>RL</strong></li></ul></li></ul></li></ul></li></ul><h4 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h4><p><a href="https://arxiv.org/pdf/2412.09413">Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems</a></p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
